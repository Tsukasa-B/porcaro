# source/porcaro_rl/porcaro_rl/tasks/direct/porcaro_rl/agents/rsl_rl_ppo_cfg.py

from isaaclab.utils import configclass

# ğŸ’¡ ä¿®æ­£ç‚¹1: RecurrentCfgã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã¾ãŸã¯ç½®ãæ›ãˆï¼‰
from isaaclab_rl.rsl_rl import (
    RslRlOnPolicyRunnerCfg, 
    RslRlPpoAlgorithmCfg, 
    RslRlPpoActorCriticRecurrentCfg # æ–°ã—ã„ã‚¯ãƒ©ã‚¹
) 


@configclass
class PPORunnerCfg(RslRlOnPolicyRunnerCfg):
    # â˜…å¤‰æ›´: 16 -> 48
    # 50Hzã§ç´„1ç§’åˆ†ï¼ˆ2æ‹åˆ†ï¼‰ã®æœªæ¥ã¾ã§è¦‹é€šã—ã¦ã€æ¬¡ã®å‹•ä½œã‚’è¨ˆç”»ã•ã›ã¾ã™ã€‚
    num_steps_per_env = 48
    
    # â˜…å¤‰æ›´: 150 -> 1500
    # é•·æ™‚é–“ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§å®‰å®šã—ãŸãƒªã‚ºãƒ ã‚’ç¿’å¾—ã™ã‚‹ãŸã‚ã€è©¦è¡Œå›æ•°ã‚’å¢—ã‚„ã—ã¾ã™ã€‚
    max_iterations = 1500
    
    save_interval = 50
    experiment_name = "porcaro_rslrl_recurrent_lstm_double" # åå‰ã‚’å¤‰ãˆã¦ãŠãã¨ç®¡ç†ã—ã‚„ã™ã„ã§ã™
    
    # ğŸ’¡ ä¿®æ­£ç‚¹2: Policyã‚¯ãƒ©ã‚¹ã‚’Recurrentãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«å¤‰æ›´
    policy = RslRlPpoActorCriticRecurrentCfg(
        init_noise_std=1.0,
        # RNNã‚’ä½¿ã†å ´åˆã€è¦³æ¸¬ã®æ­£è¦åŒ–ã‚’ONã«ã™ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¾ã™
        actor_obs_normalization=True, 
        critic_obs_normalization=True, 
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚µã‚¤ã‚ºã‚’å¤§ãã‚ã«è¨­å®š (RNNã®éš ã‚Œå±¤ã‚µã‚¤ã‚ºã¨æƒãˆã‚‹ã“ã¨ãŒå¤šã„)
        actor_hidden_dims=[128, 128], 
        critic_hidden_dims=[128, 128], 
        activation="elu",
        
        # ğŸ’¡ ä¿®æ­£ç‚¹3: RNNé–¢é€£ã®å¼•æ•°ã‚’è¨­å®š
        rnn_type="lstm", # or "gru"
        rnn_hidden_dim=128,
        rnn_num_layers=1,
    )
    algorithm = RslRlPpoAlgorithmCfg(
        value_loss_coef=1.0,
        use_clipped_value_loss=True,
        clip_param=0.2,
        entropy_coef=0.005,
        num_learning_epochs=5,
        num_mini_batches=4,
        learning_rate=1.0e-3,
        schedule="adaptive",
        gamma=0.99,
        lam=0.95,
        desired_kl=0.01,
        max_grad_norm=1.0,
    )